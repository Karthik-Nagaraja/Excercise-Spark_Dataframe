{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"females\": 1994141, \"total\": 4079669, \"males\": 2085528, \"age\": 0, \"year\": 2010}\n",
      "\n",
      "{\"females\": 1997991, \"total\": 4085341, \"males\": 2087350, \"age\": 1, \"year\": 2010}\n",
      "\n",
      "{\"females\": 2000746, \"total\": 4089295, \"males\": 2088549, \"age\": 2, \"year\": 2010}\n",
      "\n",
      "{\"females\": 2002756, \"total\": 4092221, \"males\": 2089465, \"age\": 3, \"year\": 2010}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('census_2010.json')\n",
    "\n",
    "for i in range(0,4):\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD is essentially a list of tuples with no enforced schema or structure of any kind. An RDD can have a variable number of elements in each tuple, and combinations of types between tuples.\n",
    "\n",
    "RDDs are useful for representing unstructured data like text. Without them, we'd need to write a lot of custom Python code to interact with such data.\n",
    "\n",
    "We use the SparkContext object to read data into an RDD:\n",
    "\n",
    "raw_data = sc.textFile(\\\"daily_show.tsv\\\")\n",
    "\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "\n",
    "To use the familar DataFrame query interface from pandas, however, the data representation needs to include rows, columns, and types. Spark's implementation of DataFrames mirrors the pandas implementation, with logic for rows and columns.\n",
    "\n",
    "The Spark SQL class is very powerful. It gives Spark more information about the data structure we're using and the computations we want to perform. Spark uses that information to optimize processes.\n",
    "\n",
    "To take advantage of these features, we'll have to use the SQLContext object to structure external data as a DataFrame, instead of the SparkContext object.\n",
    "\n",
    "We can query Spark DataFrame objects with SQL, which we'll explore in the next mission. The SQLContext class gets its name from this capability.\n",
    "\n",
    "This class allows us to read in data and create new DataFrames from a wide range of sources. It can do this because it takes advantage of Spark's powerful Data Sources API.\n",
    "\n",
    "File Formats\n",
    "\n",
    "    JSON, CSV/TSV, XML\n",
    "    Parquet, Amazon S3 (cloud storage service)\n",
    "\n",
    "Big Data Systems\n",
    "\n",
    "    Hive, Avro, HBase\n",
    "\n",
    "SQL Database Systems\n",
    "\n",
    "    MySQL, PostgreSQL\n",
    "\n",
    "Data science organizations often use a wide range of systems to collect and store data, and they're constantly making changes to those systems. Spark DataFrames allow us to interface with different types of data, and ensure that our analysis logic will still work as the data storage mechanisms change.\n",
    "\n",
    "Now that you've learned a bit about Spark DataFrames, let's read in census_2010.json. This data set contains valid JSON on each line, which is what Spark needs in order to read the data in properly.\n",
    "\n",
    "In the following code cell, we:\n",
    "\n",
    "    Import SQLContext from the pyspark.sql library\n",
    "    Instantiate the SQLContext object (which requires the SparkContext object (sc) as a parameter), and assign it to the variable sqlCtx\n",
    "    Use the SQLContext method read.json() to read the JSON data set into a Spark DataFrame object named df\n",
    "    Print df's data type to confirm that we successfully read it in as a Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Pass in the SparkContext object `sc`\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# Read JSON data into a DataFrame object `df`\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "\n",
    "# Print the type\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema\n",
    "\n",
    "When we read data into the SQLContext object, Spark:\n",
    "\n",
    "    Instantiates a Spark DataFrame object\n",
    "    Infers the schema from the data and associates it with the DataFrame\n",
    "    Reads in the data and distributes it across clusters (if multiple clusters are available)\n",
    "    Returns the DataFrame object\n",
    "\n",
    "We expect the DataFrame Spark created to have the following columns, which were the keys in the JSON data set:\n",
    "\n",
    "    age\n",
    "    females\n",
    "    males\n",
    "    total\n",
    "    year\n",
    "\n",
    "Spark has its own type system that's similar to the pandas type system. To create a DataFrame, Spark iterates over the data set twice - once to extract the structure of the columns, and once to infer each column's type. Let's use one of the Spark DataFrame instance methods to display the schema for the DataFrame we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlCtx = SQLContext(sc)\n",
    "df = sqlCtx.read.json(\"census_2010.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas vs Spark DataFrames\n",
    "\n",
    "As we mentioned before, the pandas DataFrame heavily influenced the Spark DataFrame implementation. Here are some of the methods we can find in both:\n",
    "\n",
    "    agg()\n",
    "    join()\n",
    "    sort()\n",
    "    where()\n",
    "\n",
    "Unlike pandas DataFrames, however, Spark DataFrames are immutable, which means we can't modify existing objects. Most transformations on an object return a new DataFrame reflecting the changes instead. As we discussed in previous missions, Spark's creators deliberately designed immutability into Spark to make it easier to work with distributed data structures.\n",
    "\n",
    "Pandas and Spark DataFrames also have different underlying data structures. Pandas DataFrames are built around Series objects, while Spark DataFrames are built around RDDs. We can perform most of the same computations and transformations on Spark DataFrames that we can on pandas DataFrames, but the styles and methods are somewhat different. We'll explore how to perform common pandas functions with Spark in this mission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.show() # Shows first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Objects\n",
    "\n",
    "In pandas, we used the head() method to return the first n rows. This is one of the differences between the DataFrame implementations. Instead of returning a nicely formatted table of values, the head() method in Spark returns a list of row objects. Spark needs to return row objects for certain methods, such as head(), collect() and take().\n",
    "\n",
    "You can access a row's attributes by the column name using dot notation, and by position using bracket notation with an index:\n",
    "\n",
    "row_one = df.head(5)[0]\n",
    "\n",
    "#Access value for age\n",
    "\n",
    "row_one.age\n",
    "\n",
    "#Access the first value\n",
    "\n",
    "row_one[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
